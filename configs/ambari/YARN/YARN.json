{
    "properties": {
        "capacity-scheduler": {
            "properties": {
                "capacity-scheduler": "null",
                "yarn.scheduler.capacity.default.minimum-user-limit-percent": "100",
                "yarn.scheduler.capacity.maximum-am-resource-percent": "0.2",
                "yarn.scheduler.capacity.maximum-applications": "10000",
                "yarn.scheduler.capacity.node-locality-delay": "40",
                "yarn.scheduler.capacity.resource-calculator": "org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator",
                "yarn.scheduler.capacity.root.accessible-node-labels": "*",
                "yarn.scheduler.capacity.root.acl_administer_queue": "*",
                "yarn.scheduler.capacity.root.capacity": "100",
                "yarn.scheduler.capacity.root.default.acl_administer_jobs": "*",
                "yarn.scheduler.capacity.root.default.acl_submit_applications": "*",
                "yarn.scheduler.capacity.root.default.capacity": "100",
                "yarn.scheduler.capacity.root.default.maximum-capacity": "100",
                "yarn.scheduler.capacity.root.default.state": "RUNNING",
                "yarn.scheduler.capacity.root.default.user-limit-factor": "1",
                "yarn.scheduler.capacity.root.queues": "default"
            },
            "properties_attributes": {}
        },
        "mapred-env": {
            "properties": {
                "content": "\n# export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n\nexport HADOOP_JOB_HISTORYSERVER_HEAPSIZE={{jobhistory_heapsize}}\n\nexport HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA\n\n#export HADOOP_JOB_HISTORYSERVER_OPTS=\n#export HADOOP_MAPRED_LOG_DIR=\"\" # Where log files are stored.  $HADOOP_MAPRED_HOME/logs by default.\n#export HADOOP_JHS_LOGGER=INFO,RFA # Hadoop JobSummary logger.\n#export HADOOP_MAPRED_PID_DIR= # The pid files are stored. /tmp by default.\n#export HADOOP_MAPRED_IDENT_STRING= #A string representing this instance of hadoop. $USER by default\n#export HADOOP_MAPRED_NICENESS= #The scheduling priority for daemons. Defaults to 0.\nexport HADOOP_OPTS=\"-Dhdp.version=$HDP_VERSION $HADOOP_OPTS\"\nexport HADOOP_OPTS=\"-Djava.io.tmpdir={{hadoop_java_io_tmpdir}} $HADOOP_OPTS\"\nexport JAVA_LIBRARY_PATH=\"${JAVA_LIBRARY_PATH}:{{hadoop_java_io_tmpdir}}\"",
                "jobhistory_heapsize": "900",
                "mapred_log_dir_prefix": "/var/log/hadoop-mapreduce",
                "mapred_pid_dir_prefix": "/var/run/hadoop-mapreduce",
                "mapred_user": "mapred",
                "mapred_user_nofile_limit": "32768",
                "mapred_user_nproc_limit": "65536"
            },
            "properties_attributes": {}
        },
        "mapred-site": {
            "properties": {
                "mapreduce.admin.map.child.java.opts": "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
                "mapreduce.admin.reduce.child.java.opts": "-server -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true -Dhdp.version=${hdp.version}",
                "mapreduce.admin.user.env": "LD_LIBRARY_PATH=/usr/hdp/${hdp.version}/hadoop/lib/native:/usr/hdp/${hdp.version}/hadoop/lib/native/Linux-amd64-64",
                "mapreduce.am.max-attempts": "2",
                "mapreduce.application.classpath": "$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*:/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure",
                "mapreduce.application.framework.path": "/hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework",
                "mapreduce.cluster.administrators": " hadoop",
                "mapreduce.framework.name": "yarn",
                "mapreduce.job.counters.max": "130",
                "mapreduce.job.emit-timeline-data": "false",
                "mapreduce.job.queuename": "default",
                "mapreduce.job.reduce.slowstart.completedmaps": "0.05",
                "mapreduce.jobhistory.address": "%HOSTGROUP::host_group_3%:10020",
                "mapreduce.jobhistory.bind-host": "0.0.0.0",
                "mapreduce.jobhistory.done-dir": "/mr-history/done",
                "mapreduce.jobhistory.http.policy": "HTTP_ONLY",
                "mapreduce.jobhistory.intermediate-done-dir": "/mr-history/tmp",
                "mapreduce.jobhistory.recovery.enable": "true",
                "mapreduce.jobhistory.recovery.store.class": "org.apache.hadoop.mapreduce.v2.hs.HistoryServerLeveldbStateStoreService",
                "mapreduce.jobhistory.recovery.store.leveldb.path": "/hadoop/mapreduce/jhs",
                "mapreduce.jobhistory.webapp.address": "%HOSTGROUP::host_group_3%:19888",
                "mapreduce.map.java.opts": "-Xmx409m",
                "mapreduce.map.log.level": "INFO",
                "mapreduce.map.memory.mb": "512",
                "mapreduce.map.output.compress": "false",
                "mapreduce.map.sort.spill.percent": "0.7",
                "mapreduce.map.speculative": "false",
                "mapreduce.output.fileoutputformat.compress": "false",
                "mapreduce.output.fileoutputformat.compress.type": "BLOCK",
                "mapreduce.reduce.input.buffer.percent": "0.0",
                "mapreduce.reduce.java.opts": "-Xmx819m",
                "mapreduce.reduce.log.level": "INFO",
                "mapreduce.reduce.memory.mb": "1024",
                "mapreduce.reduce.shuffle.fetch.retry.enabled": "1",
                "mapreduce.reduce.shuffle.fetch.retry.interval-ms": "1000",
                "mapreduce.reduce.shuffle.fetch.retry.timeout-ms": "30000",
                "mapreduce.reduce.shuffle.input.buffer.percent": "0.7",
                "mapreduce.reduce.shuffle.merge.percent": "0.66",
                "mapreduce.reduce.shuffle.parallelcopies": "30",
                "mapreduce.reduce.speculative": "false",
                "mapreduce.shuffle.port": "13562",
                "mapreduce.task.io.sort.factor": "100",
                "mapreduce.task.io.sort.mb": "286",
                "mapreduce.task.timeout": "300000",
                "yarn.app.mapreduce.am.admin-command-opts": "-Dhdp.version=${hdp.version}",
                "yarn.app.mapreduce.am.command-opts": "-Xmx409m -Dhdp.version=${hdp.version}",
                "yarn.app.mapreduce.am.log.level": "INFO",
                "yarn.app.mapreduce.am.resource.mb": "512",
                "yarn.app.mapreduce.am.staging-dir": "/user"
            },
            "properties_attributes": {}
        },
        "ranger-yarn-audit": {
            "properties": {
                "xasecure.audit.destination.hdfs": "true",
                "xasecure.audit.destination.hdfs.batch.filespool.dir": "/var/log/hadoop/yarn/audit/hdfs/spool",
                "xasecure.audit.destination.hdfs.dir": "hdfs://NAMENODE_HOSTNAME:8020/ranger/audit",
                "xasecure.audit.destination.solr": "false",
                "xasecure.audit.destination.solr.batch.filespool.dir": "/var/log/hadoop/yarn/audit/solr/spool",
                "xasecure.audit.destination.solr.urls": "",
                "xasecure.audit.destination.solr.zookeepers": "NONE",
                "xasecure.audit.is.enabled": "true",
                "xasecure.audit.provider.summary.enabled": "false"
            },
            "properties_attributes": {}
        },
        "ranger-yarn-plugin-properties": {
            "properties": {
                "REPOSITORY_CONFIG_USERNAME": "yarn",
                "common.name.for.certificate": "",
                "hadoop.rpc.protection": "",
                "policy_user": "ambari-qa",
                "ranger-yarn-plugin-enabled": "No"
            },
            "properties_attributes": {}
        },
        "ranger-yarn-policymgr-ssl": {
            "properties": {
                "xasecure.policymgr.clientssl.keystore": "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-keystore.jks",
                "xasecure.policymgr.clientssl.keystore.credential.file": "jceks://file{{credential_file}}",
                "xasecure.policymgr.clientssl.truststore": "/usr/hdp/current/hadoop-client/conf/ranger-yarn-plugin-truststore.jks",
                "xasecure.policymgr.clientssl.truststore.credential.file": "jceks://file{{credential_file}}"
            },
            "properties_attributes": {}
        },
        "ranger-yarn-security": {
            "properties": {
                "ranger.plugin.yarn.policy.cache.dir": "/etc/ranger/{{repo_name}}/policycache",
                "ranger.plugin.yarn.policy.pollIntervalMs": "30000",
                "ranger.plugin.yarn.policy.rest.ssl.config.file": "/etc/hadoop/conf/ranger-policymgr-ssl-yarn.xml",
                "ranger.plugin.yarn.policy.rest.url": "{{policymgr_mgr_url}}",
                "ranger.plugin.yarn.policy.source.impl": "org.apache.ranger.admin.client.RangerAdminRESTClient",
                "ranger.plugin.yarn.service.name": "{{repo_name}}"
            },
            "properties_attributes": {}
        },
        "yarn-env": {
            "properties": {
                "apptimelineserver_heapsize": "1024",
                "content": "\n      export HADOOP_YARN_HOME={{hadoop_yarn_home}}\n      export YARN_LOG_DIR={{yarn_log_dir_prefix}}/$USER\n      export YARN_PID_DIR={{yarn_pid_dir_prefix}}/$USER\n      export HADOOP_LIBEXEC_DIR={{hadoop_libexec_dir}}\n      export JAVA_HOME={{java64_home}}\n      export JAVA_LIBRARY_PATH=\"${JAVA_LIBRARY_PATH}:{{hadoop_java_io_tmpdir}}\"\n\n      # We need to add the EWMA appender for the yarn daemons only;\n      # however, YARN_ROOT_LOGGER is shared by the yarn client and the\n      # daemons. This is restrict the EWMA appender to daemons only.\n      INVOKER=\"${0##*/}\"\n      if [ \"$INVOKER\" == \"yarn-daemon.sh\" ]; then\n        export YARN_ROOT_LOGGER=${YARN_ROOT_LOGGER:-INFO,EWMA,RFA}\n      fi\n\n      # User for YARN daemons\n      export HADOOP_YARN_USER=${HADOOP_YARN_USER:-yarn}\n\n      # resolve links - $0 may be a softlink\n      export YARN_CONF_DIR=\"${YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf}\"\n\n      # some Java parameters\n      # export JAVA_HOME=/home/y/libexec/jdk1.6.0/\n      if [ \"$JAVA_HOME\" != \"\" ]; then\n      #echo \"run java in $JAVA_HOME\"\n      JAVA_HOME=$JAVA_HOME\n      fi\n\n      if [ \"$JAVA_HOME\" = \"\" ]; then\n      echo \"Error: JAVA_HOME is not set.\"\n      exit 1\n      fi\n\n      JAVA=$JAVA_HOME/bin/java\n      JAVA_HEAP_MAX=-Xmx1000m\n\n      # For setting YARN specific HEAP sizes please use this\n      # Parameter and set appropriately\n      YARN_HEAPSIZE={{yarn_heapsize}}\n\n      # check envvars which might override default args\n      if [ \"$YARN_HEAPSIZE\" != \"\" ]; then\n      JAVA_HEAP_MAX=\"-Xmx\"\"$YARN_HEAPSIZE\"\"m\"\n      fi\n\n      # Resource Manager specific parameters\n\n      # Specify the max Heapsize for the ResourceManager using a numerical value\n      # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n      # the value to 1000.\n      # This value will be overridden by an Xmx setting specified in either YARN_OPTS\n      # and/or YARN_RESOURCEMANAGER_OPTS.\n      # If not specified, the default value will be picked from either YARN_HEAPMAX\n      # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\n      export YARN_RESOURCEMANAGER_HEAPSIZE={{resourcemanager_heapsize}}\n\n      # Specify the JVM options to be used when starting the ResourceManager.\n      # These options will be appended to the options specified as YARN_OPTS\n      # and therefore may override any similar flags set in YARN_OPTS\n      #export YARN_RESOURCEMANAGER_OPTS=\n\n      # Node Manager specific parameters\n\n      # Specify the max Heapsize for the NodeManager using a numerical value\n      # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n      # the value to 1000.\n      # This value will be overridden by an Xmx setting specified in either YARN_OPTS\n      # and/or YARN_NODEMANAGER_OPTS.\n      # If not specified, the default value will be picked from either YARN_HEAPMAX\n      # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\n      export YARN_NODEMANAGER_HEAPSIZE={{nodemanager_heapsize}}\n\n      # Specify the max Heapsize for the timeline server using a numerical value\n      # in the scale of MB. For example, to specify an jvm option of -Xmx1000m, set\n      # the value to 1024.\n      # This value will be overridden by an Xmx setting specified in either YARN_OPTS\n      # and/or YARN_TIMELINESERVER_OPTS.\n      # If not specified, the default value will be picked from either YARN_HEAPMAX\n      # or JAVA_HEAP_MAX with YARN_HEAPMAX as the preferred option of the two.\n      export YARN_TIMELINESERVER_HEAPSIZE={{apptimelineserver_heapsize}}\n\n      # Specify the JVM options to be used when starting the NodeManager.\n      # These options will be appended to the options specified as YARN_OPTS\n      # and therefore may override any similar flags set in YARN_OPTS\n      #export YARN_NODEMANAGER_OPTS=\n\n      # so that filenames w/ spaces are handled correctly in loops below\n      IFS=\n\n\n      # default log directory and file\n      if [ \"$YARN_LOG_DIR\" = \"\" ]; then\n      YARN_LOG_DIR=\"$HADOOP_YARN_HOME/logs\"\n      fi\n      if [ \"$YARN_LOGFILE\" = \"\" ]; then\n      YARN_LOGFILE='yarn.log'\n      fi\n\n      # default policy file for service-level authorization\n      if [ \"$YARN_POLICYFILE\" = \"\" ]; then\n      YARN_POLICYFILE=\"hadoop-policy.xml\"\n      fi\n\n      # restore ordinary behaviour\n      unset IFS\n\n\n      YARN_OPTS=\"$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR\"\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR\"\n      YARN_OPTS=\"$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE\"\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE\"\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME\"\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING\"\n      YARN_OPTS=\"$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}\"\n      export YARN_NODEMANAGER_OPTS=\"$YARN_NODEMANAGER_OPTS -Dnm.audit.logger=INFO,NMAUDIT\"\n      export YARN_RESOURCEMANAGER_OPTS=\"$YARN_RESOURCEMANAGER_OPTS -Drm.audit.logger=INFO,RMAUDIT\"\n      if [ \"x$JAVA_LIBRARY_PATH\" != \"x\" ]; then\n      YARN_OPTS=\"$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH\"\n      fi\n      YARN_OPTS=\"$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE\"\n      YARN_OPTS=\"$YARN_OPTS -Djava.io.tmpdir={{hadoop_java_io_tmpdir}}\"",
                "is_supported_yarn_ranger": "true",
                "min_user_id": "1000",
                "nodemanager_heapsize": "1024",
                "resourcemanager_heapsize": "1024",
                "service_check.queue.name": "default",
                "yarn_cgroups_enabled": "false",
                "yarn_heapsize": "1024",
                "yarn_log_dir_prefix": "/var/log/hadoop-yarn",
                "yarn_pid_dir_prefix": "/var/run/hadoop-yarn",
                "yarn_user": "yarn",
                "yarn_user_nofile_limit": "32768",
                "yarn_user_nproc_limit": "65536"
            },
            "properties_attributes": {}
        },
        "yarn-log4j": {
            "properties": {
                "content": "\n#Relative to Yarn Log Dir Prefix\nyarn.log.dir=.\n#\n# Job Summary Appender\n#\n# Use following logger to send summary to separate file defined by\n# hadoop.mapreduce.jobsummary.log.file rolled daily:\n# hadoop.mapreduce.jobsummary.logger=INFO,JSA\n#\nhadoop.mapreduce.jobsummary.logger=${hadoop.root.logger}\nhadoop.mapreduce.jobsummary.log.file=hadoop-mapreduce.jobsummary.log\nlog4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender\n# Set the ResourceManager summary log filename\nyarn.server.resourcemanager.appsummary.log.file=hadoop-mapreduce.jobsummary.log\n# Set the ResourceManager summary log level and appender\nyarn.server.resourcemanager.appsummary.logger=${hadoop.root.logger}\n#yarn.server.resourcemanager.appsummary.logger=INFO,RMSUMMARY\n\n# To enable AppSummaryLogging for the RM,\n# set yarn.server.resourcemanager.appsummary.logger to\n# LEVEL,RMSUMMARY in hadoop-env.sh\n\n# Appender for ResourceManager Application Summary Log\n# Requires the following properties to be set\n#    - hadoop.log.dir (Hadoop Log directory)\n#    - yarn.server.resourcemanager.appsummary.log.file (resource manager app summary log filename)\n#    - yarn.server.resourcemanager.appsummary.logger (resource manager app summary log level and appender)\nlog4j.appender.RMSUMMARY=org.apache.log4j.RollingFileAppender\nlog4j.appender.RMSUMMARY.File=${yarn.log.dir}/${yarn.server.resourcemanager.appsummary.log.file}\nlog4j.appender.RMSUMMARY.MaxFileSize=256MB\nlog4j.appender.RMSUMMARY.MaxBackupIndex=20\nlog4j.appender.RMSUMMARY.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMSUMMARY.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.appender.JSA.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{2}: %m%n\nlog4j.appender.JSA.DatePattern=.yyyy-MM-dd\nlog4j.appender.JSA.layout=org.apache.log4j.PatternLayout\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=${yarn.server.resourcemanager.appsummary.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager$ApplicationSummary=false\n\n# Appender for viewing information for errors and warnings\nyarn.ewma.cleanupInterval=300\nyarn.ewma.messageAgeLimitSeconds=86400\nyarn.ewma.maxUniqueMessages=250\nlog4j.appender.EWMA=org.apache.hadoop.yarn.util.Log4jWarningErrorMetricsAppender\nlog4j.appender.EWMA.cleanupInterval=${yarn.ewma.cleanupInterval}\nlog4j.appender.EWMA.messageAgeLimitSeconds=${yarn.ewma.messageAgeLimitSeconds}\nlog4j.appender.EWMA.maxUniqueMessages=${yarn.ewma.maxUniqueMessages}\n\n# Audit logging for ResourceManager\nrm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=${rm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger=false\nlog4j.appender.RMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.RMAUDIT.File=${yarn.log.dir}/rm-audit.log\nlog4j.appender.RMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.RMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.RMAUDIT.DatePattern=.yyyy-MM-dd\n\n# Audit logging for NodeManager\nnm.audit.logger=${hadoop.root.logger}\nlog4j.logger.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=${nm.audit.logger}\nlog4j.additivity.org.apache.hadoop.yarn.server.nodemanager.NMAuditLogger=false\nlog4j.appender.NMAUDIT=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.NMAUDIT.File=${yarn.log.dir}/nm-audit.log\nlog4j.appender.NMAUDIT.layout=org.apache.log4j.PatternLayout\nlog4j.appender.NMAUDIT.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n\nlog4j.appender.NMAUDIT.DatePattern=.yyyy-MM-dd"
            },
            "properties_attributes": {}
        },
        "yarn-site": {
            "properties": {
                "hadoop.registry.rm.enabled": "true",
                "hadoop.registry.zk.quorum": "%HOSTGROUP::host_group_5%:2181",
                "yarn.acl.enable": "false",
                "yarn.admin.acl": "yarn",
                "yarn.application.classpath": "$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*",
                "yarn.client.nodemanager-connect.max-wait-ms": "60000",
                "yarn.client.nodemanager-connect.retry-interval-ms": "10000",
                "yarn.http.policy": "HTTP_ONLY",
                "yarn.log-aggregation-enable": "true",
                "yarn.log-aggregation.retain-seconds": "2592000",
                "yarn.log.server.url": "http://%HOSTGROUP::host_group_3%:19888/jobhistory/logs",
                "yarn.node-labels.enabled": "false",
                "yarn.node-labels.fs-store.retry-policy-spec": "2000, 500",
                "yarn.node-labels.fs-store.root-dir": "/system/yarn/node-labels",
                "yarn.nodemanager.address": "0.0.0.0:45454",
                "yarn.nodemanager.admin-env": "MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX",
                "yarn.nodemanager.aux-services": "mapreduce_shuffle,spark_shuffle,spark2_shuffle",
                "yarn.nodemanager.aux-services.mapreduce_shuffle.class": "org.apache.hadoop.mapred.ShuffleHandler",
                "yarn.nodemanager.aux-services.spark2_shuffle.class": "org.apache.spark.network.yarn.YarnShuffleService",
                "yarn.nodemanager.aux-services.spark2_shuffle.classpath": "/usr/hdp/${hdp.version}/spark2/aux/*",
                "yarn.nodemanager.aux-services.spark_shuffle.class": "org.apache.spark.network.yarn.YarnShuffleService",
                "yarn.nodemanager.aux-services.spark_shuffle.classpath": "/usr/hdp/${hdp.version}/spark/aux/*",
                "yarn.nodemanager.bind-host": "0.0.0.0",
                "yarn.nodemanager.container-executor.class": "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
                "yarn.nodemanager.container-monitor.interval-ms": "3000",
                "yarn.nodemanager.delete.debug-delay-sec": "0",
                "yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage": "90",
                "yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb": "1000",
                "yarn.nodemanager.disk-health-checker.min-healthy-disks": "0.25",
                "yarn.nodemanager.health-checker.interval-ms": "135000",
                "yarn.nodemanager.health-checker.script.timeout-ms": "60000",
                "yarn.nodemanager.linux-container-executor.cgroups.hierarchy": "hadoop-yarn",
                "yarn.nodemanager.linux-container-executor.cgroups.mount": "false",
                "yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage": "false",
                "yarn.nodemanager.linux-container-executor.group": "hadoop",
                "yarn.nodemanager.linux-container-executor.resources-handler.class": "org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler",
                "yarn.nodemanager.local-dirs": "/hadoop/yarn/local",
                "yarn.nodemanager.log-aggregation.compression-type": "gz",
                "yarn.nodemanager.log-aggregation.debug-enabled": "false",
                "yarn.nodemanager.log-aggregation.num-log-files-per-app": "30",
                "yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds": "-1",
                "yarn.nodemanager.log-dirs": "/hadoop/yarn/log",
                "yarn.nodemanager.log.retain-second": "604800",
                "yarn.nodemanager.recovery.dir": "{{yarn_log_dir_prefix}}/nodemanager/recovery-state",
                "yarn.nodemanager.recovery.enabled": "true",
                "yarn.nodemanager.remote-app-log-dir": "/app-logs",
                "yarn.nodemanager.remote-app-log-dir-suffix": "logs",
                "yarn.nodemanager.resource.cpu-vcores": "1",
                "yarn.nodemanager.resource.memory-mb": "2048",
                "yarn.nodemanager.resource.percentage-physical-cpu-limit": "80",
                "yarn.nodemanager.vmem-check-enabled": "false",
                "yarn.nodemanager.vmem-pmem-ratio": "2.1",
                "yarn.resourcemanager.address": "%HOSTGROUP::host_group_3%:8050",
                "yarn.resourcemanager.admin.address": "%HOSTGROUP::host_group_3%:8141",
                "yarn.resourcemanager.am.max-attempts": "2",
                "yarn.resourcemanager.bind-host": "0.0.0.0",
                "yarn.resourcemanager.connect.max-wait.ms": "900000",
                "yarn.resourcemanager.connect.retry-interval.ms": "30000",
                "yarn.resourcemanager.fs.state-store.retry-policy-spec": "2000, 500",
                "yarn.resourcemanager.fs.state-store.uri": " ",
                "yarn.resourcemanager.ha.enabled": "false",
                "yarn.resourcemanager.hostname": "%HOSTGROUP::host_group_3%",
                "yarn.resourcemanager.nodes.exclude-path": "/etc/hadoop/conf/yarn.exclude",
                "yarn.resourcemanager.recovery.enabled": "true",
                "yarn.resourcemanager.resource-tracker.address": "%HOSTGROUP::host_group_3%:8025",
                "yarn.resourcemanager.scheduler.address": "%HOSTGROUP::host_group_3%:8030",
                "yarn.resourcemanager.scheduler.class": "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                "yarn.resourcemanager.scheduler.monitor.enable": "false",
                "yarn.resourcemanager.state-store.max-completed-applications": "${yarn.resourcemanager.max-completed-applications}",
                "yarn.resourcemanager.store.class": "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
                "yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size": "10",
                "yarn.resourcemanager.system-metrics-publisher.enabled": "true",
                "yarn.resourcemanager.webapp.address": "%HOSTGROUP::host_group_3%:8088",
                "yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled": "false",
                "yarn.resourcemanager.webapp.https.address": "%HOSTGROUP::host_group_3%:8090",
                "yarn.resourcemanager.work-preserving-recovery.enabled": "true",
                "yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms": "10000",
                "yarn.resourcemanager.zk-acl": "world:anyone:rwcda",
                "yarn.resourcemanager.zk-address": "%HOSTGROUP::host_group_5%:2181",
                "yarn.resourcemanager.zk-num-retries": "1000",
                "yarn.resourcemanager.zk-retry-interval-ms": "1000",
                "yarn.resourcemanager.zk-state-store.parent-path": "/rmstore",
                "yarn.resourcemanager.zk-timeout-ms": "10000",
                "yarn.scheduler.maximum-allocation-mb": "2048",
                "yarn.scheduler.maximum-allocation-vcores": "1",
                "yarn.scheduler.minimum-allocation-mb": "512",
                "yarn.scheduler.minimum-allocation-vcores": "1",
                "yarn.timeline-service.address": "%HOSTGROUP::host_group_3%:10200",
                "yarn.timeline-service.bind-host": "0.0.0.0",
                "yarn.timeline-service.client.max-retries": "30",
                "yarn.timeline-service.client.retry-interval-ms": "1000",
                "yarn.timeline-service.enabled": "true",
                "yarn.timeline-service.entity-group-fs-store.active-dir": "/ats/active/",
                "yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds": "3600",
                "yarn.timeline-service.entity-group-fs-store.done-dir": "/ats/done/",
                "yarn.timeline-service.entity-group-fs-store.group-id-plugin-classes": "org.apache.tez.dag.history.logging.ats.TimelineCachePluginImpl",
                "yarn.timeline-service.entity-group-fs-store.retain-seconds": "604800",
                "yarn.timeline-service.entity-group-fs-store.scan-interval-seconds": "60",
                "yarn.timeline-service.entity-group-fs-store.summary-store": "org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore",
                "yarn.timeline-service.generic-application-history.store-class": "org.apache.hadoop.yarn.server.applicationhistoryservice.NullApplicationHistoryStore",
                "yarn.timeline-service.http-authentication.proxyuser.root.groups": "*",
                "yarn.timeline-service.http-authentication.proxyuser.root.hosts": "hdp-ambari-server-1",
                "yarn.timeline-service.http-authentication.simple.anonymous.allowed": "true",
                "yarn.timeline-service.http-authentication.type": "simple",
                "yarn.timeline-service.leveldb-state-store.path": "/hadoop/yarn/timeline",
                "yarn.timeline-service.leveldb-timeline-store.path": "/hadoop/yarn/timeline",
                "yarn.timeline-service.leveldb-timeline-store.read-cache-size": "104857600",
                "yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size": "10000",
                "yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size": "10000",
                "yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms": "300000",
                "yarn.timeline-service.recovery.enabled": "true",
                "yarn.timeline-service.state-store-class": "org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore",
                "yarn.timeline-service.store-class": "org.apache.hadoop.yarn.server.timeline.EntityGroupFSTimelineStore",
                "yarn.timeline-service.ttl-enable": "true",
                "yarn.timeline-service.ttl-ms": "2678400000",
                "yarn.timeline-service.version": "1.5",
                "yarn.timeline-service.webapp.address": "%HOSTGROUP::host_group_3%:8188",
                "yarn.timeline-service.webapp.https.address": "%HOSTGROUP::host_group_3%:8190"
            },
            "properties_attributes": {}
        }
    }
}